{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca9d30cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "def generate_path_API_1D(model, lake, start, stop, variables=\"T\"):\n",
    "    \"\"\"\n",
    "    Generate API path for 1D simulation data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : str\n",
    "        The simulation model (e.g., 'simstrat')\n",
    "    lake : str\n",
    "        The lake name (e.g., 'aegeri')\n",
    "    start : str\n",
    "        Start datetime in format 'YYYYMMDDHHMM' (e.g., '202405050300')\n",
    "    stop : str\n",
    "        Stop datetime in format 'YYYYMMDDHHMM' (e.g., '202406072300')\n",
    "    variables : str, optional\n",
    "        Variables to query (default: 'T' for temperature)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Complete API URL path\n",
    "    \"\"\"\n",
    "    base_url = \"https://alplakes-api.eawag.ch/simulations/1d/depthtime\"\n",
    "    return f\"{base_url}/{model}/{lake}/{start}/{stop}?variables={variables}\"\n",
    "\n",
    "\n",
    "def read_API_1D_to_dataframe(api_url_or_json_path, variable='T'):\n",
    "    \"\"\"\n",
    "    Read API JSON data and convert to pandas DataFrame with datetime index.\n",
    "    \n",
    "    This function reads JSON data from either:\n",
    "    - A URL (API endpoint)\n",
    "    - A local JSON file path\n",
    "    \n",
    "    The JSON structure should contain:\n",
    "    - 'time': list of datetime strings\n",
    "    - 'depth': dict with 'data' (list of depth values), 'unit', 'description'\n",
    "    - 'variables': dict with variable names as keys (e.g., 'T' for temperature)\n",
    "      Each variable contains 'data' (2D array), 'unit', 'description'\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    api_url_or_json_path : str\n",
    "        Either a URL to the API endpoint or a path to a local JSON file\n",
    "    variable : str, optional\n",
    "        Variable name to extract (default: 'T' for temperature)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with:\n",
    "        - First column: 'Datetime' (timezone-naive datetime64[ns])\n",
    "        - Remaining columns: Depth values (e.g., '-81.000' to '-0.000')\n",
    "        - Values: Temperature or other variable data\n",
    "        \n",
    "    Note: Datetime values are converted to timezone-naive for better compatibility\n",
    "          with numpy and most pandas operations.\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> # From API\n",
    "    >>> url = generate_path_API_1D('simstrat', 'aegeri', '202405050300', '202406072300')\n",
    "    >>> df = read_API_1D_to_dataframe(url)\n",
    "    >>> print(df.columns)\n",
    "    Index(['Datetime', '-81.000', '-80.000', ..., '-1.000', '-0.000'], dtype='object')\n",
    "    \n",
    "    >>> # From local file\n",
    "    >>> df = read_API_1D_to_dataframe('ttt.json')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load JSON data\n",
    "    if api_url_or_json_path.startswith('http'):\n",
    "        # Fetch from API with timeout\n",
    "        try:\n",
    "            response = requests.get(api_url_or_json_path, timeout=300)  # 5 minute timeout\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "        except requests.exceptions.Timeout:\n",
    "            raise TimeoutError(\n",
    "                \"API request timed out. The date range might be too large. \"\n",
    "                \"Try requesting smaller time periods (e.g., one year at a time).\"\n",
    "            )\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response.status_code == 504:\n",
    "                raise TimeoutError(\n",
    "                    f\"API gateway timeout (504). The date range is too large. \"\n",
    "                    f\"Try requesting smaller time periods (e.g., one year at a time) or use fetch_API_1D_chunked().\"\n",
    "                )\n",
    "            else:\n",
    "                raise\n",
    "    else:\n",
    "        # Load from local file\n",
    "        with open(api_url_or_json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    \n",
    "    # Extract time, depth, and variable data\n",
    "    time = data['time']\n",
    "    depth_values = data['depth']['data']\n",
    "    variable_data = data['variables'][variable]['data']\n",
    "    \n",
    "    # Convert depth values to negative (representing depth below surface)\n",
    "    # Format with 3 decimal places\n",
    "    depth_values_negative = [f\"{-d:.3f}\" for d in depth_values]\n",
    "    \n",
    "    # The API returns data as [depth x time], we need [time x depth]\n",
    "    # So we need to transpose the data\n",
    "    variable_data_transposed = np.array(variable_data).T\n",
    "    \n",
    "    # Create DataFrame\n",
    "    # Rows = time points, Columns = depth values (negative, 3 decimals)\n",
    "    df = pd.DataFrame(variable_data_transposed, columns=depth_values_negative)\n",
    "    \n",
    "    # Convert time strings to datetime and remove timezone (convert to naive datetime)\n",
    "    datetime_series = pd.to_datetime(time).tz_localize(None)\n",
    "    \n",
    "    # Add Datetime as a column (not index)\n",
    "    df.insert(0, 'Datetime', datetime_series)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def fetch_API_1D_chunked(model, lake, start, stop, variables=\"T\", chunk_months=12, save_csv=None):\n",
    "    \"\"\"\n",
    "    Fetch large date ranges from API in smaller chunks and combine them.\n",
    "    \n",
    "    This function automatically splits large date ranges into smaller chunks\n",
    "    to avoid API timeouts.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : str\n",
    "        The simulation model (e.g., 'simstrat')\n",
    "    lake : str\n",
    "        The lake name (e.g., 'geneva')\n",
    "    start : str\n",
    "        Start datetime in format 'YYYYMMDDHHMM' (e.g., '198101010000')\n",
    "    stop : str\n",
    "        Stop datetime in format 'YYYYMMDDHHMM' (e.g., '202406072300')\n",
    "    variables : str, optional\n",
    "        Variables to query (default: 'T' for temperature)\n",
    "    chunk_months : int, optional\n",
    "        Number of months per chunk (default: 12 for yearly chunks)\n",
    "    save_csv : str, optional\n",
    "        Path to save the resulting DataFrame as CSV (default: None, no save)\n",
    "        Example: 'output.csv' or '/path/to/file.csv'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Combined DataFrame with all data\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> # Fetch 43 years of data in yearly chunks\n",
    "    >>> df = fetch_API_1D_chunked('simstrat', 'geneva', '198101010000', '202406072300')\n",
    "    \n",
    "    >>> # Fetch and save to CSV\n",
    "    >>> df = fetch_API_1D_chunked('simstrat', 'geneva', '198101010000', '202406072300', \n",
    "    ...                            save_csv='geneva_1981_2024.csv')\n",
    "    \"\"\"\n",
    "    from datetime import datetime, timedelta\n",
    "    from dateutil.relativedelta import relativedelta\n",
    "    \n",
    "    # Parse start and stop dates\n",
    "    start_dt = datetime.strptime(start, '%Y%m%d%H%M')\n",
    "    stop_dt = datetime.strptime(stop, '%Y%m%d%H%M')\n",
    "    \n",
    "    dfs = []\n",
    "    current_start = start_dt\n",
    "    \n",
    "    print(f\"Fetching data from {start_dt} to {stop_dt} in {chunk_months}-month chunks...\")\n",
    "    \n",
    "    chunk_count = 0\n",
    "    while current_start < stop_dt:\n",
    "        # Calculate chunk end (either chunk_months ahead or final stop date)\n",
    "        current_stop = min(\n",
    "            current_start + relativedelta(months=chunk_months),\n",
    "            stop_dt\n",
    "        )\n",
    "        \n",
    "        # Format dates back to API format\n",
    "        chunk_start_str = current_start.strftime('%Y%m%d%H%M')\n",
    "        chunk_stop_str = current_stop.strftime('%Y%m%d%H%M')\n",
    "        \n",
    "        # Generate URL and fetch\n",
    "        url = generate_path_API_1D(model, lake, chunk_start_str, chunk_stop_str, variables)\n",
    "        \n",
    "        chunk_count += 1\n",
    "        print(f\"  Chunk {chunk_count}: {current_start.date()} to {current_stop.date()}...\", end=' ')\n",
    "        \n",
    "        try:\n",
    "            df_chunk = read_API_1D_to_dataframe(url, variable=variables)\n",
    "            dfs.append(df_chunk)\n",
    "            print(f\"✓ ({len(df_chunk)} rows)\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error: {e}\")\n",
    "            # Continue with next chunk even if this one fails\n",
    "        \n",
    "        # Move to next chunk\n",
    "        current_start = current_stop\n",
    "    \n",
    "    if not dfs:\n",
    "        raise ValueError(\"No data was successfully fetched\")\n",
    "    \n",
    "    # Combine all chunks\n",
    "    print(f\"\\nCombining {len(dfs)} chunks...\")\n",
    "    df_combined = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "    \n",
    "    # Remove any duplicate timestamps (at chunk boundaries)\n",
    "    df_combined = df_combined.drop_duplicates(subset=['Datetime'], keep='first')\n",
    "    \n",
    "    # Reset index after dropping duplicates\n",
    "    df_combined = df_combined.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"✓ Total: {len(df_combined)} rows\")\n",
    "    \n",
    "    # Save to CSV if requested\n",
    "    if save_csv:\n",
    "        print(f\"\\nSaving to CSV: {save_csv}...\")\n",
    "        # Create directory if it doesn't exist\n",
    "        import os\n",
    "        csv_dir = os.path.dirname(save_csv)\n",
    "        if csv_dir and not os.path.exists(csv_dir):\n",
    "            os.makedirs(csv_dir, exist_ok=True)\n",
    "            print(f\"  Created directory: {csv_dir}\")\n",
    "        df_combined.to_csv(save_csv, index=False)\n",
    "        print(f\"✓ Saved successfully\")\n",
    "    \n",
    "    return df_combined\n",
    "\n",
    "\n",
    "def load_lake_data(model, lake, start, stop, variables=\"T\", \n",
    "                   local_path=None, save_csv=None, chunk_months=12, \n",
    "                   force_download=False):\n",
    "    \"\"\"\n",
    "    Smart function to load lake simulation data from local file or API.\n",
    "    \n",
    "    This function automatically decides whether to:\n",
    "    1. Load from a local file (if it exists and is not stale)\n",
    "    2. Download from the API (if local file doesn't exist or is outdated)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : str\n",
    "        The simulation model (e.g., 'simstrat')\n",
    "    lake : str\n",
    "        The lake name (e.g., 'geneva', 'aegeri')\n",
    "    start : str\n",
    "        Start datetime in format 'YYYYMMDDHHMM'\n",
    "    stop : str\n",
    "        Stop datetime in format 'YYYYMMDDHHMM'\n",
    "    variables : str, optional\n",
    "        Variables to query (default: 'T' for temperature)\n",
    "    local_path : str, optional\n",
    "        Path to check for local data file. Can be:\n",
    "        - CSV file: 'data/geneva/temperature.csv'\n",
    "        - DAT file: 'data/geneva/T_out.dat'\n",
    "        - Directory: 'data/geneva' (will look for T_out.dat or *.csv)\n",
    "        If None, always downloads from API\n",
    "    save_csv : str, optional\n",
    "        Path to save downloaded data as CSV (default: None)\n",
    "    chunk_months : int, optional\n",
    "        Number of months per chunk when downloading (default: 12)\n",
    "    force_download : bool, optional\n",
    "        If True, always download from API even if local file exists (default: False)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with lake simulation data\n",
    "    \n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> # Try local first, download if needed\n",
    "    >>> df = load_lake_data('simstrat', 'geneva', '198101010000', '202406072300',\n",
    "    ...                      local_path='data/Geneva/T_out.dat')\n",
    "    \n",
    "    >>> # Always download and save\n",
    "    >>> df = load_lake_data('simstrat', 'geneva', '198101010000', '202406072300',\n",
    "    ...                      force_download=True, save_csv='geneva_new.csv')\n",
    "    \n",
    "    >>> # Download if no local file exists\n",
    "    >>> df = load_lake_data('simstrat', 'aegeri', '202001010000', '202406072300',\n",
    "    ...                      local_path='data/aegeri/', save_csv='data/aegeri/temperature.csv')\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Determine if we should try to load from local file\n",
    "    should_load_local = False\n",
    "    local_file_path = None\n",
    "    \n",
    "    if local_path and not force_download:\n",
    "        if os.path.isdir(local_path):\n",
    "            # Check for common filenames in the directory\n",
    "            possible_files = [\n",
    "                os.path.join(local_path, 'T_out.dat'),\n",
    "                os.path.join(local_path, 'temperature.csv'),\n",
    "                os.path.join(local_path, f'{lake}_temperature.csv'),\n",
    "            ]\n",
    "            for file_path in possible_files:\n",
    "                if os.path.exists(file_path):\n",
    "                    local_file_path = file_path\n",
    "                    should_load_local = True\n",
    "                    break\n",
    "        elif os.path.isfile(local_path):\n",
    "            local_file_path = local_path\n",
    "            should_load_local = True\n",
    "    \n",
    "    # Load from local file if available\n",
    "    if should_load_local:\n",
    "        print(f\"Loading data from local file: {local_file_path}\")\n",
    "        file_ext = os.path.splitext(local_file_path)[1].lower()\n",
    "        \n",
    "        if file_ext == '.csv':\n",
    "            # Load CSV file\n",
    "            df = pd.read_csv(local_file_path)\n",
    "            if 'Datetime' in df.columns:\n",
    "                df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "            print(f\"✓ Loaded {len(df)} rows from CSV\")\n",
    "            \n",
    "        elif file_ext == '.dat':\n",
    "            # Load DAT file (Simstrat format)\n",
    "            # Format: First column is days since reference date\n",
    "            df = pd.read_csv(local_file_path)\n",
    "            \n",
    "            # Get the time column (first column)\n",
    "            time_col = df.columns[0]\n",
    "            time_values = df[time_col]\n",
    "            \n",
    "            # Get depth columns (all columns except first)\n",
    "            depth_columns = df.columns[1:].tolist()\n",
    "            \n",
    "            # Convert first column (days) to datetime\n",
    "            # Assuming reference date is 1981-01-01 (common for Simstrat)\n",
    "            datetime_values = pd.Timestamp('1981-01-01') + pd.to_timedelta(time_values, unit='D')\n",
    "            \n",
    "            # Create new column names with proper formatting\n",
    "            new_columns = ['Datetime']\n",
    "            for col in depth_columns:\n",
    "                try:\n",
    "                    depth = float(col)\n",
    "                    # Assume positive values in DAT file should be negative (depth below surface)\n",
    "                    if depth >= 0:\n",
    "                        new_columns.append(f'-{depth:.3f}')\n",
    "                    else:\n",
    "                        new_columns.append(f'{depth:.3f}')\n",
    "                except:\n",
    "                    # Keep non-numeric column names as-is\n",
    "                    new_columns.append(col)\n",
    "            \n",
    "            # Create new dataframe with proper structure\n",
    "            data_dict = {'Datetime': datetime_values}\n",
    "            for i, old_col in enumerate(depth_columns):\n",
    "                data_dict[new_columns[i+1]] = df[old_col].values\n",
    "            \n",
    "            df = pd.DataFrame(data_dict)\n",
    "            \n",
    "            print(f\"✓ Loaded {len(df)} rows from DAT file\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_ext}. Use .csv or .dat\")\n",
    "        \n",
    "        # Filter by date range\n",
    "        start_dt = pd.to_datetime(start, format='%Y%m%d%H%M')\n",
    "        stop_dt = pd.to_datetime(stop, format='%Y%m%d%H%M')\n",
    "        \n",
    "        df = df[(df['Datetime'] >= start_dt) & (df['Datetime'] <= stop_dt)]\n",
    "        print(f\"✓ Filtered to date range: {len(df)} rows between {start_dt.date()} and {stop_dt.date()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # Download from API\n",
    "    else:\n",
    "        if force_download:\n",
    "            print(f\"Force download enabled - fetching from API...\")\n",
    "        elif local_path:\n",
    "            print(f\"Local file not found at: {local_path}\")\n",
    "            print(f\"Downloading from API...\")\n",
    "        else:\n",
    "            print(f\"No local path specified - downloading from API...\")\n",
    "        \n",
    "        # Create save directory if needed\n",
    "        if save_csv:\n",
    "            csv_dir = os.path.dirname(save_csv)\n",
    "            if csv_dir and not os.path.exists(csv_dir):\n",
    "                os.makedirs(csv_dir, exist_ok=True)\n",
    "                print(f\"✓ Created directory: {csv_dir}\")\n",
    "        \n",
    "        df = fetch_API_1D_chunked(\n",
    "            model=model,\n",
    "            lake=lake,\n",
    "            start=start,\n",
    "            stop=stop,\n",
    "            variables=variables,\n",
    "            chunk_months=chunk_months,\n",
    "            save_csv=save_csv\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c54eac2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local file not found at: data/Aegeri\n",
      "Downloading from API...\n",
      "✓ Created directory: data/Aegeri\n",
      "Fetching data from 2019-01-01 00:00:00 to 2021-01-01 00:00:00 in 12-month chunks...\n",
      "  Chunk 1: 2019-01-01 to 2020-01-01... ✓ (2920 rows)\n",
      "  Chunk 2: 2020-01-01 to 2021-01-01... ✓ (2928 rows)\n",
      "\n",
      "Combining 2 chunks...\n",
      "✓ Total: 5848 rows\n",
      "\n",
      "Saving to CSV: data/Aegeri/aegeri.csv...\n",
      "✓ Saved successfully\n"
     ]
    }
   ],
   "source": [
    "df = load_lake_data(\n",
    "    model='simstrat',\n",
    "    lake='aegeri',\n",
    "    start='201901010000',      # ✓ String, 12 digits\n",
    "    stop='202101010000',       # ✓ String, 12 digits  \n",
    "    variables=\"T\",\n",
    "    local_path='data/Aegeri',  # Will check here first\n",
    "    save_csv='data/Aegeri/aegeri.csv',  # Directory created automatically! ✓\n",
    "    chunk_months=12,\n",
    "    force_download=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d28344e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>-81.000</th>\n",
       "      <th>-80.000</th>\n",
       "      <th>-79.000</th>\n",
       "      <th>-78.000</th>\n",
       "      <th>-77.000</th>\n",
       "      <th>-76.000</th>\n",
       "      <th>-75.000</th>\n",
       "      <th>-74.000</th>\n",
       "      <th>-73.000</th>\n",
       "      <th>...</th>\n",
       "      <th>-9.000</th>\n",
       "      <th>-8.000</th>\n",
       "      <th>-7.000</th>\n",
       "      <th>-6.000</th>\n",
       "      <th>-5.000</th>\n",
       "      <th>-4.000</th>\n",
       "      <th>-3.000</th>\n",
       "      <th>-2.000</th>\n",
       "      <th>-1.000</th>\n",
       "      <th>-0.000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01 01:00:02.880</td>\n",
       "      <td>5.952</td>\n",
       "      <td>5.952</td>\n",
       "      <td>5.952</td>\n",
       "      <td>5.952</td>\n",
       "      <td>5.952</td>\n",
       "      <td>5.951</td>\n",
       "      <td>5.951</td>\n",
       "      <td>5.950</td>\n",
       "      <td>5.950</td>\n",
       "      <td>...</td>\n",
       "      <td>5.897</td>\n",
       "      <td>5.896</td>\n",
       "      <td>5.894</td>\n",
       "      <td>5.893</td>\n",
       "      <td>5.891</td>\n",
       "      <td>5.890</td>\n",
       "      <td>5.888</td>\n",
       "      <td>5.885</td>\n",
       "      <td>5.882</td>\n",
       "      <td>5.876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01 04:00:02.880</td>\n",
       "      <td>5.949</td>\n",
       "      <td>5.949</td>\n",
       "      <td>5.949</td>\n",
       "      <td>5.949</td>\n",
       "      <td>5.949</td>\n",
       "      <td>5.948</td>\n",
       "      <td>5.948</td>\n",
       "      <td>5.948</td>\n",
       "      <td>5.947</td>\n",
       "      <td>...</td>\n",
       "      <td>5.890</td>\n",
       "      <td>5.888</td>\n",
       "      <td>5.886</td>\n",
       "      <td>5.885</td>\n",
       "      <td>5.883</td>\n",
       "      <td>5.881</td>\n",
       "      <td>5.879</td>\n",
       "      <td>5.876</td>\n",
       "      <td>5.872</td>\n",
       "      <td>5.866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01 07:00:02.880</td>\n",
       "      <td>5.947</td>\n",
       "      <td>5.946</td>\n",
       "      <td>5.946</td>\n",
       "      <td>5.946</td>\n",
       "      <td>5.946</td>\n",
       "      <td>5.945</td>\n",
       "      <td>5.945</td>\n",
       "      <td>5.945</td>\n",
       "      <td>5.944</td>\n",
       "      <td>...</td>\n",
       "      <td>5.883</td>\n",
       "      <td>5.882</td>\n",
       "      <td>5.880</td>\n",
       "      <td>5.878</td>\n",
       "      <td>5.876</td>\n",
       "      <td>5.875</td>\n",
       "      <td>5.872</td>\n",
       "      <td>5.870</td>\n",
       "      <td>5.866</td>\n",
       "      <td>5.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01 10:00:02.880</td>\n",
       "      <td>5.944</td>\n",
       "      <td>5.944</td>\n",
       "      <td>5.944</td>\n",
       "      <td>5.943</td>\n",
       "      <td>5.943</td>\n",
       "      <td>5.943</td>\n",
       "      <td>5.942</td>\n",
       "      <td>5.942</td>\n",
       "      <td>5.942</td>\n",
       "      <td>...</td>\n",
       "      <td>5.883</td>\n",
       "      <td>5.882</td>\n",
       "      <td>5.881</td>\n",
       "      <td>5.880</td>\n",
       "      <td>5.879</td>\n",
       "      <td>5.878</td>\n",
       "      <td>5.877</td>\n",
       "      <td>5.876</td>\n",
       "      <td>5.874</td>\n",
       "      <td>5.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01 13:00:02.880</td>\n",
       "      <td>5.942</td>\n",
       "      <td>5.942</td>\n",
       "      <td>5.942</td>\n",
       "      <td>5.941</td>\n",
       "      <td>5.941</td>\n",
       "      <td>5.941</td>\n",
       "      <td>5.940</td>\n",
       "      <td>5.940</td>\n",
       "      <td>5.940</td>\n",
       "      <td>...</td>\n",
       "      <td>5.893</td>\n",
       "      <td>5.894</td>\n",
       "      <td>5.894</td>\n",
       "      <td>5.895</td>\n",
       "      <td>5.896</td>\n",
       "      <td>5.897</td>\n",
       "      <td>5.897</td>\n",
       "      <td>5.897</td>\n",
       "      <td>5.896</td>\n",
       "      <td>5.892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5843</th>\n",
       "      <td>2020-12-31 10:00:02.880</td>\n",
       "      <td>6.021</td>\n",
       "      <td>6.021</td>\n",
       "      <td>6.021</td>\n",
       "      <td>6.020</td>\n",
       "      <td>6.020</td>\n",
       "      <td>6.020</td>\n",
       "      <td>6.019</td>\n",
       "      <td>6.018</td>\n",
       "      <td>6.018</td>\n",
       "      <td>...</td>\n",
       "      <td>5.925</td>\n",
       "      <td>5.923</td>\n",
       "      <td>5.922</td>\n",
       "      <td>5.921</td>\n",
       "      <td>5.919</td>\n",
       "      <td>5.918</td>\n",
       "      <td>5.916</td>\n",
       "      <td>5.914</td>\n",
       "      <td>5.911</td>\n",
       "      <td>5.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5844</th>\n",
       "      <td>2020-12-31 13:00:02.880</td>\n",
       "      <td>6.016</td>\n",
       "      <td>6.016</td>\n",
       "      <td>6.016</td>\n",
       "      <td>6.016</td>\n",
       "      <td>6.015</td>\n",
       "      <td>6.015</td>\n",
       "      <td>6.014</td>\n",
       "      <td>6.014</td>\n",
       "      <td>6.013</td>\n",
       "      <td>...</td>\n",
       "      <td>5.936</td>\n",
       "      <td>5.936</td>\n",
       "      <td>5.935</td>\n",
       "      <td>5.935</td>\n",
       "      <td>5.934</td>\n",
       "      <td>5.934</td>\n",
       "      <td>5.933</td>\n",
       "      <td>5.932</td>\n",
       "      <td>5.931</td>\n",
       "      <td>5.927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5845</th>\n",
       "      <td>2020-12-31 16:00:02.880</td>\n",
       "      <td>6.012</td>\n",
       "      <td>6.011</td>\n",
       "      <td>6.011</td>\n",
       "      <td>6.011</td>\n",
       "      <td>6.010</td>\n",
       "      <td>6.010</td>\n",
       "      <td>6.009</td>\n",
       "      <td>6.009</td>\n",
       "      <td>6.008</td>\n",
       "      <td>...</td>\n",
       "      <td>5.937</td>\n",
       "      <td>5.935</td>\n",
       "      <td>5.934</td>\n",
       "      <td>5.933</td>\n",
       "      <td>5.931</td>\n",
       "      <td>5.929</td>\n",
       "      <td>5.927</td>\n",
       "      <td>5.924</td>\n",
       "      <td>5.920</td>\n",
       "      <td>5.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5846</th>\n",
       "      <td>2020-12-31 19:00:02.880</td>\n",
       "      <td>6.006</td>\n",
       "      <td>6.006</td>\n",
       "      <td>6.006</td>\n",
       "      <td>6.006</td>\n",
       "      <td>6.005</td>\n",
       "      <td>6.004</td>\n",
       "      <td>6.004</td>\n",
       "      <td>6.004</td>\n",
       "      <td>6.003</td>\n",
       "      <td>...</td>\n",
       "      <td>5.925</td>\n",
       "      <td>5.923</td>\n",
       "      <td>5.921</td>\n",
       "      <td>5.919</td>\n",
       "      <td>5.916</td>\n",
       "      <td>5.914</td>\n",
       "      <td>5.912</td>\n",
       "      <td>5.908</td>\n",
       "      <td>5.904</td>\n",
       "      <td>5.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5847</th>\n",
       "      <td>2020-12-31 22:00:02.880</td>\n",
       "      <td>6.001</td>\n",
       "      <td>6.001</td>\n",
       "      <td>6.001</td>\n",
       "      <td>6.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>5.999</td>\n",
       "      <td>5.998</td>\n",
       "      <td>5.998</td>\n",
       "      <td>...</td>\n",
       "      <td>5.915</td>\n",
       "      <td>5.913</td>\n",
       "      <td>5.910</td>\n",
       "      <td>5.908</td>\n",
       "      <td>5.906</td>\n",
       "      <td>5.903</td>\n",
       "      <td>5.901</td>\n",
       "      <td>5.897</td>\n",
       "      <td>5.892</td>\n",
       "      <td>5.883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5848 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Datetime  -81.000  -80.000  -79.000  -78.000  -77.000  \\\n",
       "0    2019-01-01 01:00:02.880    5.952    5.952    5.952    5.952    5.952   \n",
       "1    2019-01-01 04:00:02.880    5.949    5.949    5.949    5.949    5.949   \n",
       "2    2019-01-01 07:00:02.880    5.947    5.946    5.946    5.946    5.946   \n",
       "3    2019-01-01 10:00:02.880    5.944    5.944    5.944    5.943    5.943   \n",
       "4    2019-01-01 13:00:02.880    5.942    5.942    5.942    5.941    5.941   \n",
       "...                      ...      ...      ...      ...      ...      ...   \n",
       "5843 2020-12-31 10:00:02.880    6.021    6.021    6.021    6.020    6.020   \n",
       "5844 2020-12-31 13:00:02.880    6.016    6.016    6.016    6.016    6.015   \n",
       "5845 2020-12-31 16:00:02.880    6.012    6.011    6.011    6.011    6.010   \n",
       "5846 2020-12-31 19:00:02.880    6.006    6.006    6.006    6.006    6.005   \n",
       "5847 2020-12-31 22:00:02.880    6.001    6.001    6.001    6.000    6.000   \n",
       "\n",
       "      -76.000  -75.000  -74.000  -73.000  ...  -9.000  -8.000  -7.000  -6.000  \\\n",
       "0       5.951    5.951    5.950    5.950  ...   5.897   5.896   5.894   5.893   \n",
       "1       5.948    5.948    5.948    5.947  ...   5.890   5.888   5.886   5.885   \n",
       "2       5.945    5.945    5.945    5.944  ...   5.883   5.882   5.880   5.878   \n",
       "3       5.943    5.942    5.942    5.942  ...   5.883   5.882   5.881   5.880   \n",
       "4       5.941    5.940    5.940    5.940  ...   5.893   5.894   5.894   5.895   \n",
       "...       ...      ...      ...      ...  ...     ...     ...     ...     ...   \n",
       "5843    6.020    6.019    6.018    6.018  ...   5.925   5.923   5.922   5.921   \n",
       "5844    6.015    6.014    6.014    6.013  ...   5.936   5.936   5.935   5.935   \n",
       "5845    6.010    6.009    6.009    6.008  ...   5.937   5.935   5.934   5.933   \n",
       "5846    6.004    6.004    6.004    6.003  ...   5.925   5.923   5.921   5.919   \n",
       "5847    6.000    5.999    5.998    5.998  ...   5.915   5.913   5.910   5.908   \n",
       "\n",
       "      -5.000  -4.000  -3.000  -2.000  -1.000  -0.000  \n",
       "0      5.891   5.890   5.888   5.885   5.882   5.876  \n",
       "1      5.883   5.881   5.879   5.876   5.872   5.866  \n",
       "2      5.876   5.875   5.872   5.870   5.866   5.860  \n",
       "3      5.879   5.878   5.877   5.876   5.874   5.870  \n",
       "4      5.896   5.897   5.897   5.897   5.896   5.892  \n",
       "...      ...     ...     ...     ...     ...     ...  \n",
       "5843   5.919   5.918   5.916   5.914   5.911   5.906  \n",
       "5844   5.934   5.934   5.933   5.932   5.931   5.927  \n",
       "5845   5.931   5.929   5.927   5.924   5.920   5.912  \n",
       "5846   5.916   5.914   5.912   5.908   5.904   5.896  \n",
       "5847   5.906   5.903   5.901   5.897   5.892   5.883  \n",
       "\n",
       "[5848 rows x 83 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a15589c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
